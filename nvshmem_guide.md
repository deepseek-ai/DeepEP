# NVSHMEM æ·±åº¦æŒ‡å—ï¼šèƒŒæ™¯çŸ¥è¯†ä¸ API å®æˆ˜

## ç›®å½•

1. [NVSHMEM ç®€ä»‹](#nvshmem-ç®€ä»‹)
2. [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
3. [å¯¹ç§°å†…å­˜è¯¦è§£](#å¯¹ç§°å†…å­˜è¯¦è§£)
4. [API ä½¿ç”¨ç¤ºä¾‹](#api-ä½¿ç”¨ç¤ºä¾‹)
5. [æ€§èƒ½å¯¹æ¯”ï¼šNVSHMEM vs MPI](#æ€§èƒ½å¯¹æ¯”nvshmem-vs-mpi)
6. [å®æˆ˜æ¡ˆä¾‹](#å®æˆ˜æ¡ˆä¾‹)

---

## NVSHMEM ç®€ä»‹

### ä»€ä¹ˆæ˜¯ NVSHMEMï¼Ÿ

**NVSHMEM** (NVIDIA Shared Memory) æ˜¯ NVIDIA å¼€å‘çš„åŸºäº OpenSHMEM æ ‡å‡†çš„å¹¶è¡Œç¼–ç¨‹æ¥å£ï¼Œä¸“ä¸º NVIDIA GPU é›†ç¾¤è®¾è®¡ï¼Œæä¾›é«˜æ•ˆä¸”å¯æ‰©å±•çš„ GPU é—´é€šä¿¡èƒ½åŠ›ã€‚

**æœ€æ–°ç‰ˆæœ¬**ï¼šNVSHMEM 3.4.5ï¼ˆæˆªè‡³ 2025 å¹´ï¼‰

### æ ¸å¿ƒç‰¹æ€§

1. **PGAS æ¨¡å‹** (Partitioned Global Address Space)
   - åœ¨å¤šä¸ª GPU çš„å†…å­˜ä¸­åˆ›å»ºå…¨å±€åœ°å€ç©ºé—´
   - å¯é€šè¿‡ç»†ç²’åº¦æ“ä½œè®¿é—®è¿œç¨‹ GPU å†…å­˜

2. **GPU ç›´æ¥å‘èµ·é€šä¿¡**
   - æ— éœ€ CPU å‚ä¸ï¼ŒGPU kernel ç›´æ¥å‘èµ·æ•°æ®ä¼ è¾“
   - æ¶ˆé™¤ CPU-GPU åŒæ­¥å¼€é”€

3. **å¼‚æ­¥é€šä¿¡**
   - ä¸ MPI çš„é˜»å¡å¼ send/recv ä¸åŒ
   - ä½¿ç”¨å¼‚æ­¥ã€å•è¾¹é€šä¿¡åŸè¯­

4. **æ˜“ç”¨çš„å¯¹ç§°å†…å­˜åˆ†é…**
   - æä¾›ç®€å•æ¥å£åˆ†é…è·¨ GPU å¯¹ç§°åˆ†å¸ƒçš„å†…å­˜

### NVSHMEM vs ä¼ ç»Ÿ MPI

| ç‰¹æ€§ | NVSHMEM | CUDA-aware MPI |
|------|---------|----------------|
| **é€šä¿¡å‘èµ·è€…** | GPU kernel | CPU |
| **åŒæ­¥å¼€é”€** | æ—  CPU-GPU åŒæ­¥ | éœ€è¦ CPU-GPU åŒæ­¥ |
| **é€šä¿¡æ¨¡å¼** | å•è¾¹ï¼ˆPut/Getï¼‰ | åŒè¾¹ï¼ˆSend/Recvï¼‰ |
| **ç»†ç²’åº¦é€šä¿¡** | æ”¯æŒï¼ˆthread çº§ï¼‰ | ä¸æ”¯æŒ |
| **ç¼–ç¨‹æ¨¡å‹** | PGAS | æ¶ˆæ¯ä¼ é€’ |

---

## æ ¸å¿ƒæ¦‚å¿µ

### 1. PE (Processing Element)

åœ¨ NVSHMEM ä¸­ï¼Œæ¯ä¸ª GPU ç§°ä¸ºä¸€ä¸ª **PE**ã€‚

```cpp
int my_pe = nvshmem_my_pe();        // è·å–å½“å‰ PE çš„ ID
int n_pes = nvshmem_n_pes();        // è·å–æ€» PE æ•°é‡
```

**ç¤ºä¾‹**ï¼š
```
4 ä¸ª GPU é›†ç¾¤ï¼š
  PE 0: GPU 0
  PE 1: GPU 1
  PE 2: GPU 2
  PE 3: GPU 3
```

### 2. å¯¹ç§°å¯¹è±¡ (Symmetric Objects)

**å¯¹ç§°å¯¹è±¡**æ˜¯åœ¨æ‰€æœ‰ PE çš„å¯¹ç§°å †ï¼ˆsymmetric heapï¼‰ä¸Šåˆ†é…çš„å†…å­˜ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹æ€§ï¼š

- åœ¨æ‰€æœ‰ PE ä¸Šçš„**è™šæ‹Ÿåœ°å€ç›¸åŒ**
- å¯ä»¥è¢«ä»»ä½• PE çš„ GPU kernel ç›´æ¥è®¿é—®
- ä½¿ç”¨ PE ID + å¯¹ç§°åœ°å€è®¿é—®è¿œç¨‹å†…å­˜

### 3. å…¨å±€åœ°å€ç©ºé—´

```
PE 0 å†…å­˜           PE 1 å†…å­˜           PE 2 å†…å­˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç§æœ‰å†…å­˜ â”‚       â”‚ ç§æœ‰å†…å­˜ â”‚       â”‚ ç§æœ‰å†…å­˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ å¯¹ç§°å †   â”‚â—„â”€â”€â”€â”€â”€â–ºâ”‚ å¯¹ç§°å †   â”‚â—„â”€â”€â”€â”€â”€â–ºâ”‚ å¯¹ç§°å †   â”‚
â”‚ 0x7000.. â”‚       â”‚ 0x7000.. â”‚       â”‚ 0x7000.. â”‚
â”‚          â”‚       â”‚          â”‚       â”‚          â”‚
â”‚ data[0]  â”‚       â”‚ data[1]  â”‚       â”‚ data[2]  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â–²                  â–²                  â–²
     â”‚                  â”‚                  â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          å¯è·¨ PE ç›´æ¥è®¿é—®ï¼ˆç›¸åŒè™šæ‹Ÿåœ°å€ï¼‰
```

---

## å¯¹ç§°å†…å­˜è¯¦è§£

### å¯¹ç§°å †åˆ†é…ç­–ç•¥

NVSHMEM æ”¯æŒä¸¤ç§å¯¹ç§°å †åˆ†é…ç­–ç•¥ï¼š

#### 1. åŠ¨æ€åˆ†é…ï¼ˆé»˜è®¤ï¼Œä½¿ç”¨ CUDA VMMï¼‰

```bash
# å¯ç”¨åŠ¨æ€åˆ†é…ï¼ˆé»˜è®¤ï¼‰
export NVSHMEM_DISABLE_CUDA_VMM=0
```

**ä¼˜åŠ¿**ï¼š
- æŒ‰éœ€åˆ†é…ï¼Œçµæ´»é«˜æ•ˆ
- ä½¿ç”¨ CUDA Virtual Memory Management (VMM) API
- æ— éœ€é¢„å…ˆæŒ‡å®šå †å¤§å°

#### 2. é™æ€åˆ†é…

```bash
# ç¦ç”¨ CUDA VMMï¼Œä½¿ç”¨é™æ€åˆ†é…
export NVSHMEM_DISABLE_CUDA_VMM=1

# æŒ‡å®šå¯¹ç§°å †å¤§å°ï¼ˆä¾‹å¦‚ 4 GBï¼‰
export NVSHMEM_SYMMETRIC_SIZE=4294967296
```

**ä¼˜åŠ¿**ï¼š
- é¢„åˆ†é…ï¼Œå¯åŠ¨æ—¶å›ºå®š
- é€‚ç”¨äºä¸æ”¯æŒ VMM çš„æ—§æ¶æ„

---

### å¯¹ç§°å†…å­˜åˆ†é… API

#### nvshmem_malloc - å¯¹ç§°å †åˆ†é…

```cpp
void* nvshmem_malloc(size_t size);
```

**ç‰¹æ€§**ï¼š
- **é›†ä½“æ“ä½œ**ï¼ˆCollectiveï¼‰ï¼šæ‰€æœ‰ PE å¿…é¡»åŒæ—¶è°ƒç”¨
- è¿”å›å¯¹ç§°åœ°å€ï¼Œåœ¨æ‰€æœ‰ PE ä¸Šè™šæ‹Ÿåœ°å€ç›¸åŒ
- ä»å¯¹ç§°å †åˆ†é…ï¼ˆvs `malloc` ä»ç§æœ‰å †åˆ†é…ï¼‰

**ç¤ºä¾‹ 1ï¼šåŸºæœ¬åˆ†é…**

```cpp
#include <nvshmem.h>
#include <nvshmemx.h>

__global__ void init_kernel(int* data, int my_pe) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    if (tid == 0) {
        data[0] = my_pe * 100;  // æ¯ä¸ª PE å†™å…¥ä¸åŒçš„å€¼
    }
}

int main() {
    // åˆå§‹åŒ– NVSHMEM
    nvshmem_init();

    int my_pe = nvshmem_my_pe();
    int n_pes = nvshmem_n_pes();

    // åœ¨å¯¹ç§°å †ä¸Šåˆ†é… 1024 ä¸ªæ•´æ•°ï¼ˆæ‰€æœ‰ PE å¿…é¡»è°ƒç”¨ï¼‰
    int* symmetric_data = (int*) nvshmem_malloc(1024 * sizeof(int));

    // åˆå§‹åŒ–æ•°æ®
    init_kernel<<<1, 256>>>(symmetric_data, my_pe);
    cudaDeviceSynchronize();

    // è®¿é—®è¿œç¨‹ PE çš„æ•°æ®ï¼ˆè§åç»­ç¤ºä¾‹ï¼‰

    // é‡Šæ”¾å¯¹ç§°å†…å­˜ï¼ˆé›†ä½“æ“ä½œï¼‰
    nvshmem_free(symmetric_data);

    nvshmem_finalize();
    return 0;
}
```

**å…³é”®ç‚¹**ï¼š
- `nvshmem_malloc` æ˜¯**é›†ä½“æ“ä½œ**ï¼Œæ‰€æœ‰ PE å¿…é¡»ä¼ é€’**ç›¸åŒçš„ size**
- è¿”å›çš„æŒ‡é’ˆåœ¨æ‰€æœ‰ PE ä¸Šè™šæ‹Ÿåœ°å€ç›¸åŒ
- å¿…é¡»ä½¿ç”¨ `nvshmem_free` é‡Šæ”¾ï¼ˆä¹Ÿæ˜¯é›†ä½“æ“ä½œï¼‰

---

#### nvshmem_align - å¯¹é½åˆ†é…

```cpp
void* nvshmem_align(size_t alignment, size_t size);
```

**ç”¨é€”**ï¼šåˆ†é…å¯¹é½çš„å¯¹ç§°å†…å­˜ï¼ˆä¾‹å¦‚ 128 å­—èŠ‚å¯¹é½ï¼‰

**ç¤ºä¾‹**ï¼š

```cpp
// åˆ†é… 128 å­—èŠ‚å¯¹é½çš„ 4096 å­—èŠ‚å¯¹ç§°å†…å­˜
void* aligned_data = nvshmem_align(128, 4096);
```

---

#### nvshmem_free - é‡Šæ”¾å¯¹ç§°å†…å­˜

```cpp
void nvshmem_free(void* ptr);
```

**æ³¨æ„**ï¼š
- **é›†ä½“æ“ä½œ**ï¼Œæ‰€æœ‰ PE å¿…é¡»è°ƒç”¨
- åªèƒ½é‡Šæ”¾ç”± `nvshmem_malloc` æˆ– `nvshmem_align` åˆ†é…çš„å†…å­˜

---

### é™æ€å¯¹ç§°æ•°æ®ï¼ˆå…¨å±€å˜é‡ï¼‰

é™¤äº†åŠ¨æ€åˆ†é…ï¼Œè¿˜å¯ä»¥å£°æ˜é™æ€å¯¹ç§°å¯¹è±¡ï¼š

```cpp
// å£°æ˜å¯¹ç§°å…¨å±€å˜é‡
__device__ int symmetric_counter;

__global__ void increment_kernel() {
    if (threadIdx.x == 0 && blockIdx.x == 0) {
        symmetric_counter++;
    }
}
```

**ç¼–è¯‘è¦æ±‚**ï¼š
```bash
nvcc -rdc=true -lcuda -lnvshmem -o app app.cu
```

- `-rdc=true`ï¼šå¯ç”¨å¯é‡å®šä½è®¾å¤‡ä»£ç ï¼ˆRelocatable Device Codeï¼‰
- å…è®¸ NVSHMEM è¯†åˆ«è®¾å¤‡å…¨å±€å˜é‡ä¸ºå¯¹ç§°å¯¹è±¡

---

## API ä½¿ç”¨ç¤ºä¾‹

### 1. åˆå§‹åŒ–ä¸ç»ˆæ­¢

```cpp
#include <nvshmem.h>
#include <nvshmemx.h>

int main() {
    // åˆå§‹åŒ– NVSHMEM
    nvshmem_init();

    int my_pe = nvshmem_my_pe();    // è·å–å½“å‰ PE ID
    int n_pes = nvshmem_n_pes();    // è·å–æ€» PE æ•°é‡

    printf("PE %d of %d\n", my_pe, n_pes);

    // ... åº”ç”¨é€»è¾‘ ...

    // ç»ˆæ­¢ NVSHMEM
    nvshmem_finalize();
    return 0;
}
```

---

### 2. è¿œç¨‹å†…å­˜è®¿é—®ï¼ˆRMAï¼‰

#### nvshmem_put - å†™å…¥è¿œç¨‹å†…å­˜

```cpp
void nvshmem_TYPE_put(TYPE* dest, const TYPE* source, size_t nelems, int pe);
```

**åŠŸèƒ½**ï¼šå°†æœ¬åœ°æ•°æ®æ‹·è´åˆ°è¿œç¨‹ PE çš„å¯¹ç§°å†…å­˜

**ç¤ºä¾‹ï¼šå•å‘æ•°æ®ä¼ è¾“**

```cpp
__global__ void put_kernel(int* symmetric_data, int my_pe, int n_pes) {
    int tid = threadIdx.x + blockIdx.x * blockDim.x;

    if (tid == 0 && my_pe == 0) {
        // PE 0 å°†æ•°æ®å†™å…¥ PE 1 çš„å¯¹ç§°å†…å­˜
        int local_data[4] = {100, 200, 300, 400};
        nvshmem_int_put(symmetric_data, local_data, 4, 1);
        // dest=PE 1 çš„ symmetric_data, source=local_data, nelems=4, pe=1
    }
}
```

**å˜ä½“**ï¼š
- `nvshmem_int_p(dest, value, pe)`ï¼šå†™å…¥å•ä¸ªå€¼
- `nvshmem_int_put_nbi(...)`ï¼šéé˜»å¡ç‰ˆæœ¬

---

#### nvshmem_get - ä»è¿œç¨‹è¯»å–

```cpp
void nvshmem_TYPE_get(TYPE* dest, const TYPE* source, size_t nelems, int pe);
```

**åŠŸèƒ½**ï¼šä»è¿œç¨‹ PE çš„å¯¹ç§°å†…å­˜è¯»å–æ•°æ®åˆ°æœ¬åœ°

**ç¤ºä¾‹ï¼šè¯»å–è¿œç¨‹æ•°æ®**

```cpp
__global__ void get_kernel(int* symmetric_data, int* local_buffer, int my_pe) {
    int tid = threadIdx.x + blockIdx.x * blockDim.x;

    if (tid == 0 && my_pe == 1) {
        // PE 1 ä» PE 0 è¯»å–æ•°æ®
        nvshmem_int_get(local_buffer, symmetric_data, 4, 0);
        // dest=local_buffer, source=PE 0 çš„ symmetric_data, nelems=4, pe=0
    }
}
```

**å˜ä½“**ï¼š
- `nvshmem_int_g(source, pe)`ï¼šè¯»å–å•ä¸ªå€¼å¹¶è¿”å›
- `nvshmem_int_get_nbi(...)`ï¼šéé˜»å¡ç‰ˆæœ¬

---

#### Block çº§ä¼˜åŒ– API

NVSHMEM æä¾› block çº§ APIï¼Œåˆ©ç”¨æ•´ä¸ª thread block å¹¶è¡Œæ‹·è´ï¼š

```cpp
void nvshmemx_TYPE_put_block(TYPE* dest, const TYPE* source, size_t nelems, int pe);
```

**ç¤ºä¾‹ï¼šBlock çº§ Put**

```cpp
__global__ void put_block_kernel(float* symmetric_data, float* local_data, int my_pe) {
    // æ‰€æœ‰çº¿ç¨‹å‚ä¸æ‹·è´ï¼ˆå¦‚æœç›®æ ‡ GPU æ”¯æŒ P2Pï¼‰
    if (my_pe == 0) {
        nvshmemx_float_put_block(symmetric_data, local_data, 1024, 1);
        // NVSHMEM è¿è¡Œæ—¶ä¼šåˆ©ç”¨ block å†…æ‰€æœ‰çº¿ç¨‹å¹¶è¡Œæ‹·è´
    }
}
```

**ä¼˜åŠ¿**ï¼š
- å¦‚æœç›®æ ‡ GPU é€šè¿‡ P2P è¿æ¥ï¼Œè¿è¡Œæ—¶ä¼šåˆ©ç”¨æ‰€æœ‰çº¿ç¨‹å¹¶å‘æ‹·è´
- æ˜¾è‘—æé«˜å¤§æ•°æ®ä¼ è¾“çš„å¸¦å®½

---

### 3. åŒæ­¥æ“ä½œ

#### nvshmem_barrier_all - å…¨å±€å±éšœ

```cpp
void nvshmem_barrier_all(void);
```

**åŠŸèƒ½**ï¼šæ‰€æœ‰ PE åŒæ­¥ï¼Œç±»ä¼¼ MPI_Barrier

**ç¤ºä¾‹ï¼šç¡®ä¿æ•°æ®ä¼ è¾“å®Œæˆ**

```cpp
__global__ void sync_example(int* data, int my_pe, int n_pes) {
    int tid = threadIdx.x + blockIdx.x * blockDim.x;

    if (tid == 0) {
        // PE 0 å†™å…¥æ•°æ®åˆ°æ‰€æœ‰å…¶ä»– PE
        if (my_pe == 0) {
            for (int i = 1; i < n_pes; i++) {
                nvshmem_int_p(data, 42, i);
            }
        }

        // æ‰€æœ‰ PE ç­‰å¾…æ•°æ®ä¼ è¾“å®Œæˆ
        nvshmem_barrier_all();

        // ç°åœ¨æ‰€æœ‰ PE éƒ½å¯ä»¥å®‰å…¨è¯»å– data
        printf("PE %d: data = %d\n", my_pe, *data);
    }
}
```

---

#### nvshmem_quiet - ç­‰å¾…æ‰€æœ‰ RMA å®Œæˆ

```cpp
void nvshmem_quiet(void);
```

**åŠŸèƒ½**ï¼šç­‰å¾…å½“å‰ PE å‘èµ·çš„æ‰€æœ‰ RMA æ“ä½œå®Œæˆ

**ç¤ºä¾‹**ï¼š

```cpp
__global__ void quiet_example(int* data, int my_pe) {
    if (threadIdx.x == 0) {
        // å‘èµ·å¤šä¸ª Put æ“ä½œ
        nvshmem_int_p(data, 100, (my_pe + 1) % 4);
        nvshmem_int_p(data + 1, 200, (my_pe + 2) % 4);

        // ç­‰å¾…æ‰€æœ‰ Put å®Œæˆ
        nvshmem_quiet();

        // ç°åœ¨å¯ä»¥å®‰å…¨åœ°ä¿®æ”¹æœ¬åœ°æ•°æ®
    }
}
```

---

### 4. é›†ä½“é€šä¿¡ï¼ˆCollective Operationsï¼‰

#### nvshmem_barrier - Team å±éšœ

```cpp
void nvshmem_barrier(nvshmem_team_t team);
```

**åŠŸèƒ½**ï¼šæŒ‡å®š team å†…çš„ PE åŒæ­¥

**ç¤ºä¾‹**ï¼š

```cpp
nvshmem_barrier(NVSHMEM_TEAM_WORLD);  // ç­‰ä»·äº nvshmem_barrier_all()
```

---

#### nvshmem_broadcast - å¹¿æ’­

```cpp
void nvshmem_TYPE_broadcast(nvshmem_team_t team, TYPE* dest, const TYPE* source,
                            size_t nelems, int PE_root);
```

**åŠŸèƒ½**ï¼šå°† root PE çš„æ•°æ®å¹¿æ’­åˆ° team å†…æ‰€æœ‰ PE

**ç¤ºä¾‹ï¼šPE 0 å¹¿æ’­æ•°æ®**

```cpp
__global__ void broadcast_kernel(int* data, int my_pe) {
    if (threadIdx.x == 0) {
        // PE 0 å‡†å¤‡æ•°æ®
        if (my_pe == 0) {
            data[0] = 999;
        }

        // PE 0 å¹¿æ’­åˆ°æ‰€æœ‰ PE
        nvshmem_int_broadcast(NVSHMEM_TEAM_WORLD, data, data, 1, 0);
        // team, dest, source, nelems, PE_root

        // ç°åœ¨æ‰€æœ‰ PE çš„ data[0] == 999
    }
}
```

---

#### nvshmem_alltoall - å…¨äº¤æ¢

```cpp
void nvshmem_TYPE_alltoall(nvshmem_team_t team, TYPE* dest, const TYPE* source, size_t nelems);
```

**åŠŸèƒ½**ï¼šæ¯ä¸ª PE å‘æ‰€æœ‰å…¶ä»– PE å‘é€ä¸åŒçš„æ•°æ®å—

**ç¤ºä¾‹ï¼š4 PE å…¨äº¤æ¢**

```cpp
__global__ void alltoall_kernel(int* send_buf, int* recv_buf, int my_pe, int n_pes) {
    if (threadIdx.x == 0) {
        // å‡†å¤‡å‘é€æ•°æ®
        for (int i = 0; i < n_pes; i++) {
            send_buf[i] = my_pe * 100 + i;
        }

        // å…¨äº¤æ¢ï¼ˆæ¯ä¸ª PE å‘é€ 1 ä¸ªå…ƒç´ åˆ°æ¯ä¸ª PEï¼‰
        nvshmem_int_alltoall(NVSHMEM_TEAM_WORLD, recv_buf, send_buf, 1);

        // ç»“æœï¼šrecv_buf[i] = i * 100 + my_pe
        for (int i = 0; i < n_pes; i++) {
            printf("PE %d recv from PE %d: %d\n", my_pe, i, recv_buf[i]);
        }
    }
}
```

**è¾“å‡º**ï¼ˆ4 PEsï¼‰ï¼š
```
PE 0: [0, 100, 200, 300]
PE 1: [1, 101, 201, 301]
PE 2: [2, 102, 202, 302]
PE 3: [3, 103, 203, 303]
```

---

#### nvshmem_fcollect - å…¨æ”¶é›†

```cpp
void nvshmem_TYPE_fcollect(nvshmem_team_t team, TYPE* dest, const TYPE* source, size_t nelems);
```

**åŠŸèƒ½**ï¼šæ”¶é›†æ‰€æœ‰ PE çš„æ•°æ®åˆ°æ¯ä¸ª PEï¼ˆæ‹¼æ¥ï¼‰

**ç¤ºä¾‹**ï¼š

```cpp
__global__ void fcollect_kernel(int* local_data, int* collected_data, int my_pe) {
    if (threadIdx.x == 0) {
        local_data[0] = my_pe * 10;

        // æ”¶é›†æ‰€æœ‰ PE çš„æ•°æ®
        nvshmem_int_fcollect(NVSHMEM_TEAM_WORLD, collected_data, local_data, 1);

        // æ¯ä¸ª PE çš„ collected_data: [0, 10, 20, 30, ...]
    }
}
```

---

#### Reduction æ“ä½œ

```cpp
void nvshmem_TYPE_OPERATION_reduce(nvshmem_team_t team, TYPE* dest, const TYPE* source, size_t nreduce);
```

**æ”¯æŒçš„æ“ä½œ**ï¼š`sum`, `prod`, `min`, `max`, `and`, `or`, `xor`

**ç¤ºä¾‹ï¼šæ±‚å’Œå½’çº¦**

```cpp
__global__ void reduce_kernel(int* local_val, int* result, int my_pe) {
    if (threadIdx.x == 0) {
        *local_val = my_pe + 1;  // PE i çš„å€¼ä¸º i+1

        // æ‰€æœ‰ PE æ±‚å’Œï¼Œç»“æœåœ¨æ‰€æœ‰ PE ä¸Š
        nvshmem_int_sum_reduce(NVSHMEM_TEAM_WORLD, result, local_val, 1);

        // 4 PEs: result = 1 + 2 + 3 + 4 = 10
        printf("PE %d: sum = %d\n", my_pe, *result);
    }
}
```

---

### 5. Block çº§é›†ä½“æ“ä½œ

ä½¿ç”¨ `nvshmemx_*_block` API å¯ä»¥è®©æ•´ä¸ª thread block å‚ä¸é›†ä½“æ“ä½œï¼š

```cpp
__global__ void block_collective_kernel(int* data, int my_pe) {
    // æ‰€æœ‰çº¿ç¨‹å‚ä¸ fcollect
    nvshmemx_int_fcollect_block(NVSHMEM_TEAM_WORLD, data, data, 256);
    // æ¯”å•çº¿ç¨‹ç‰ˆæœ¬å¿«å¾—å¤š
}
```

---

## æ€§èƒ½å¯¹æ¯”ï¼šNVSHMEM vs MPI

### å®æµ‹æ€§èƒ½æ•°æ®

#### GROMACS åˆ†å­åŠ¨åŠ›å­¦æ¨¡æ‹Ÿ

æ¥æºï¼š[Redesigning GROMACS Halo Exchange](https://arxiv.org/html/2509.21527v1)

| ç³»ç»Ÿè§„æ¨¡ | GPU æ•° | NVSHMEM (ns/day) | MPI (ns/day) | æå‡ |
|---------|--------|------------------|--------------|------|
| 45k atoms | 4 | **1649** | 1126 | **+46%** |
| 180k atoms | 4 | **1103** | 1058 | **+4%** |
| 180k atoms | 8 | **1249** | 973 | **+28%** |

**ç»“è®º**ï¼š
- å°ç³»ç»Ÿï¼šNVSHMEM ä¼˜åŠ¿æ˜¾è‘—ï¼ˆ**46%**ï¼‰
- å¤§ç³»ç»Ÿï¼Œå°‘ GPUï¼šMPI ç•¥ä¼˜ï¼ˆ1-3%ï¼‰
- æ‰©å±•æ€§ï¼šNVSHMEM åœ¨ 8 GPU ä¸Šä¼˜åŠ¿å¢å¤§ï¼ˆ**28%**ï¼‰

---

#### Kokkos Conjugate Gradient Solver

æ¥æºï¼šLLNL Sierra è¶…çº§è®¡ç®—æœºæµ‹è¯•

- NVSHMEM å®ç°**æ˜¾è‘—ä¼˜äº** CUDA-aware MPI
- ä»£ç é‡å¤§å¹…å‡å°‘
- GPU ç›´æ¥å‘èµ·é€šä¿¡ï¼Œæ¶ˆé™¤ CPU-GPU åŒæ­¥ç“¶é¢ˆ

---

### æ€§èƒ½ä¼˜åŠ¿åˆ†æ

#### NVSHMEM ä¼˜åŠ¿åœºæ™¯

1. **é€šä¿¡å¯†é›†å‹**
   - é¢‘ç¹çš„å°æ¶ˆæ¯é€šä¿¡
   - Halo exchangeï¼ˆè¾¹ç•Œäº¤æ¢ï¼‰
   - ç»†ç²’åº¦æ•°æ®ä¾èµ–

2. **é«˜æ‰©å±•æ€§**
   - å¤š GPU/èŠ‚ç‚¹åœºæ™¯
   - å¼ºæ‰©å±•æ€§ï¼ˆå›ºå®šé—®é¢˜è§„æ¨¡ï¼Œå¢åŠ  GPUï¼‰

3. **GPU ä¸»å¯¼è®¡ç®—**
   - æ— éœ€ CPU å‚ä¸é€šä¿¡
   - å‡å°‘ CPU-GPU æ•°æ®ç§»åŠ¨

#### MPI ä¼˜åŠ¿åœºæ™¯

1. **è®¡ç®—å¯†é›†å‹**
   - å¤§è§„æ¨¡è®¡ç®—ï¼Œå°‘é‡é€šä¿¡
   - å• GPU å¤„ç†å¤§é—®é¢˜

2. **é—ç•™ä»£ç **
   - å·²æœ‰ MPI ä»£ç åº“
   - ç§»æ¤æˆæœ¬è€ƒè™‘

---

### å…³é”®æŠ€æœ¯å·®å¼‚

| ç‰¹æ€§ | NVSHMEM | CUDA-aware MPI |
|------|---------|----------------|
| **é€šä¿¡å‘èµ·** | GPU kernel ç›´æ¥å‘èµ· | CPU å‘èµ·ï¼Œéœ€è¦ CPU-GPU åŒæ­¥ |
| **ç»†ç²’åº¦é€šä¿¡** | æ”¯æŒ thread çº§ Put/Get | ä¸æ”¯æŒï¼ˆåªèƒ½ block/grid çº§ï¼‰ |
| **å•è¾¹é€šä¿¡** | å¤©ç„¶æ”¯æŒï¼ˆPut/Getï¼‰ | éœ€è¦ MPI_Put/Getï¼ˆè¾ƒå°‘æ”¯æŒï¼‰ |
| **åŒæ­¥å¼€é”€** | æ—  CPU-GPU åŒæ­¥ | æœ‰æ˜¾è‘—åŒæ­¥å¼€é”€ |
| **ç¼–ç¨‹å¤æ‚åº¦** | PGAS æ¨¡å‹ï¼Œç›´è§‚ | æ¶ˆæ¯ä¼ é€’ï¼Œè¾ƒå¤æ‚ |

---

## å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹ 1ï¼šåˆ†å¸ƒå¼å‘é‡æ±‚å’Œ

**ç›®æ ‡**ï¼šæ¯ä¸ª PE è®¡ç®—æœ¬åœ°å‘é‡å’Œï¼Œç„¶åå…¨å±€å½’çº¦

```cpp
#include <nvshmem.h>
#include <nvshmemx.h>
#include <stdio.h>

#define N 1024

__global__ void vector_sum_kernel(float* local_vec, float* local_sum,
                                   float* global_sum, int my_pe) {
    __shared__ float shared_sum;

    int tid = threadIdx.x + blockIdx.x * blockDim.x;

    // æ¯ä¸ªçº¿ç¨‹è®¡ç®—éƒ¨åˆ†å’Œ
    float thread_sum = 0.0f;
    for (int i = tid; i < N; i += blockDim.x * gridDim.x) {
        thread_sum += local_vec[i];
    }

    // Block å†…å½’çº¦
    atomicAdd(&shared_sum, thread_sum);
    __syncthreads();

    // ç¬¬ä¸€ä¸ªçº¿ç¨‹ä¿å­˜æœ¬åœ°å’Œ
    if (threadIdx.x == 0 && blockIdx.x == 0) {
        *local_sum = shared_sum;

        // å…¨å±€æ±‚å’Œå½’çº¦
        nvshmem_float_sum_reduce(NVSHMEM_TEAM_WORLD, global_sum, local_sum, 1);
    }
}

int main() {
    nvshmem_init();

    int my_pe = nvshmem_my_pe();
    int n_pes = nvshmem_n_pes();

    // åˆ†é…å¯¹ç§°å†…å­˜
    float* local_vec = (float*) nvshmem_malloc(N * sizeof(float));
    float* local_sum = (float*) nvshmem_malloc(sizeof(float));
    float* global_sum = (float*) nvshmem_malloc(sizeof(float));

    // åˆå§‹åŒ–å‘é‡ï¼ˆæ¯ä¸ª PE çš„å€¼ä¸åŒï¼‰
    cudaMemset(local_vec, 0, N * sizeof(float));
    float init_val = (float)(my_pe + 1);
    cudaMemcpy(local_vec, &init_val, sizeof(float), cudaMemcpyHostToDevice);

    // æ‰§è¡Œæ±‚å’Œ
    vector_sum_kernel<<<4, 256>>>(local_vec, local_sum, global_sum, my_pe);
    cudaDeviceSynchronize();

    // è¯»å–ç»“æœ
    float result;
    cudaMemcpy(&result, global_sum, sizeof(float), cudaMemcpyDeviceToHost);

    if (my_pe == 0) {
        printf("Global sum: %f\n", result);
    }

    nvshmem_free(local_vec);
    nvshmem_free(local_sum);
    nvshmem_free(global_sum);

    nvshmem_finalize();
    return 0;
}
```

**ç¼–è¯‘**ï¼š
```bash
nvcc -rdc=true -I${NVSHMEM_HOME}/include -L${NVSHMEM_HOME}/lib \
     -lnvshmem -lcuda -o vector_sum vector_sum.cu
```

**è¿è¡Œ**ï¼ˆ4 GPUsï¼‰ï¼š
```bash
mpirun -np 4 ./vector_sum
```

---

### æ¡ˆä¾‹ 2ï¼šç¯å½¢é€šä¿¡ï¼ˆRing Communicationï¼‰

**ç›®æ ‡**ï¼šæ¯ä¸ª PE å‘ä¸‹ä¸€ä¸ª PE å‘é€æ•°æ®ï¼Œå½¢æˆç¯

```cpp
#include <nvshmem.h>
#include <nvshmemx.h>

__global__ void ring_kernel(int* data, int my_pe, int n_pes) {
    int tid = threadIdx.x + blockIdx.x * blockDim.x;

    if (tid == 0) {
        // åˆå§‹åŒ–æœ¬åœ°æ•°æ®
        *data = my_pe * 100;

        // å‘ä¸‹ä¸€ä¸ª PE å‘é€ï¼ˆç¯å½¢æ‹“æ‰‘ï¼‰
        int next_pe = (my_pe + 1) % n_pes;
        nvshmem_int_p(data, *data, next_pe);

        // ç­‰å¾…æ‰€æœ‰ä¼ è¾“å®Œæˆ
        nvshmem_quiet();

        // å±éšœåŒæ­¥
        nvshmem_barrier_all();

        // ç°åœ¨ data åŒ…å«å‰ä¸€ä¸ª PE çš„æ•°æ®
        printf("PE %d received: %d (from PE %d)\n",
               my_pe, *data, (my_pe - 1 + n_pes) % n_pes);
    }
}

int main() {
    nvshmem_init();

    int my_pe = nvshmem_my_pe();
    int n_pes = nvshmem_n_pes();

    int* data = (int*) nvshmem_malloc(sizeof(int));

    ring_kernel<<<1, 256>>>(data, my_pe, n_pes);
    cudaDeviceSynchronize();

    nvshmem_free(data);
    nvshmem_finalize();
    return 0;
}
```

**è¾“å‡º**ï¼ˆ4 PEsï¼‰ï¼š
```
PE 0 received: 300 (from PE 3)
PE 1 received: 0 (from PE 0)
PE 2 received: 100 (from PE 1)
PE 3 received: 200 (from PE 2)
```

---

### æ¡ˆä¾‹ 3ï¼šåˆ†å¸ƒå¼çŸ©é˜µä¹˜æ³•ï¼ˆç®€åŒ–ç‰ˆï¼‰

```cpp
#include <nvshmem.h>
#include <nvshmemx.h>

#define M 1024  // çŸ©é˜µè¡Œæ•°
#define K 1024  // å…±äº«ç»´åº¦
#define N 1024  // çŸ©é˜µåˆ—æ•°

__global__ void matmul_kernel(float* A, float* B_symmetric, float* C,
                              int my_pe, int n_pes) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < M && col < N) {
        float sum = 0.0f;

        // è®¿é—®æœ¬åœ° A çŸ©é˜µ
        for (int k = 0; k < K; k++) {
            // è®¡ç®— B çŸ©é˜µåˆ†å¸ƒåœ¨å“ªä¸ª PE
            int b_pe = (col * K + k) / (K * N / n_pes);
            int b_offset = (col * K + k) % (K * N / n_pes);

            // ä»è¿œç¨‹ PE è¯»å– B å…ƒç´ 
            float b_val = nvshmem_float_g(B_symmetric + b_offset, b_pe);

            sum += A[row * K + k] * b_val;
        }

        C[row * N + col] = sum;
    }
}

int main() {
    nvshmem_init();

    int my_pe = nvshmem_my_pe();
    int n_pes = nvshmem_n_pes();

    // åˆ†é…å†…å­˜
    float* A;
    cudaMalloc(&A, M * K * sizeof(float));  // æœ¬åœ°ç§æœ‰

    // B çŸ©é˜µåˆ†å¸ƒåœ¨å¤šä¸ª PE ä¸Šï¼ˆå¯¹ç§°å†…å­˜ï¼‰
    int b_local_size = (K * N) / n_pes;
    float* B_symmetric = (float*) nvshmem_malloc(b_local_size * sizeof(float));

    float* C;
    cudaMalloc(&C, M * N * sizeof(float));

    // åˆå§‹åŒ–çŸ©é˜µï¼ˆçœç•¥ï¼‰

    // æ‰§è¡Œåˆ†å¸ƒå¼çŸ©é˜µä¹˜æ³•
    dim3 block(16, 16);
    dim3 grid((N + 15) / 16, (M + 15) / 16);
    matmul_kernel<<<grid, block>>>(A, B_symmetric, C, my_pe, n_pes);

    cudaDeviceSynchronize();

    // æ¸…ç†
    cudaFree(A);
    nvshmem_free(B_symmetric);
    cudaFree(C);

    nvshmem_finalize();
    return 0;
}
```

---

### æ¡ˆä¾‹ 4ï¼šDeepEP ä¸­çš„ NVSHMEM ä½¿ç”¨

DeepEP ä½¿ç”¨ NVSHMEM å®ç°ä½å»¶è¿Ÿçš„ MoE dispatch/combineï¼š

```cpp
// æ¥è‡ª DeepEP çš„ç®€åŒ–ç¤ºä¾‹

// 1. åˆå§‹åŒ– NVSHMEM
nvshmemx_init_attr_t attr;
attr.mpi_comm = &mpi_comm;
nvshmemx_init_attr(NVSHMEMX_INIT_WITH_MPI_COMM, &attr);

// 2. åˆ†é…å¯¹ç§°ç¼“å†²åŒº
void* rdma_buffer = nvshmem_malloc(num_rdma_bytes);

// 3. GPU kernel ç›´æ¥ä½¿ç”¨ NVSHMEM Put
__global__ void low_latency_dispatch_kernel(...) {
    // è®¡ç®—ç›®æ ‡ PE
    int target_pe = get_target_pe(expert_id);

    // ç›´æ¥ä» GPU å‘èµ· RDMA å†™å…¥
    nvshmem_putmem_nbi(remote_addr, local_data, size, target_pe);
}

// 4. éé˜»å¡ç­‰å¾…
nvshmem_quiet();  // ç­‰å¾…æ‰€æœ‰ Put å®Œæˆ
```

**ä¼˜åŠ¿**ï¼š
- GPU kernel ç›´æ¥å‘èµ· RDMAï¼Œæ—  CPU å‚ä¸
- æ”¯æŒç»†ç²’åº¦ã€é«˜å¹¶å‘çš„é€šä¿¡
- å»¶è¿Ÿé™ä½åˆ° **77-194 å¾®ç§’**ï¼ˆvs MPI çš„æ¯«ç§’çº§ï¼‰

---

## ç¯å¢ƒå˜é‡é…ç½®

### å¸¸ç”¨ç¯å¢ƒå˜é‡

```bash
# ç¦ç”¨ CUDA VMMï¼ˆä½¿ç”¨é™æ€å¯¹ç§°å †ï¼‰
export NVSHMEM_DISABLE_CUDA_VMM=1

# è®¾ç½®é™æ€å¯¹ç§°å †å¤§å°ï¼ˆ4 GBï¼‰
export NVSHMEM_SYMMETRIC_SIZE=4294967296

# å¯ç”¨ InfiniBand IBGDAï¼ˆGPU Direct Asyncï¼‰
export NVSHMEM_IB_ENABLE_IBGDA=1

# è®¾ç½®æ¯ä¸ª PE çš„ RC QP æ•°é‡
export NVSHMEM_IBGDA_NUM_RC_PER_PE=24

# è®¾ç½® QP æ·±åº¦
export NVSHMEM_QP_DEPTH=1024

# ç¦ç”¨ P2Pï¼ˆå¼ºåˆ¶ä½¿ç”¨ IBï¼‰
export NVSHMEM_DISABLE_P2P=0

# ç¦ç”¨ NVLink SHARP
export NVSHMEM_DISABLE_NVLS=1

# è®¾ç½®è™šæ‹Ÿ laneï¼ˆæµé‡éš”ç¦»ï¼‰
export NVSHMEM_IB_SL=0

# ç¦ç”¨å¤šèŠ‚ç‚¹ NVLink
export NVSHMEM_DISABLE_MNNVL=1

# è®¾ç½®æœ€å¤§ teams æ•°é‡
export NVSHMEM_MAX_TEAMS=7
```

---

## ç¼–è¯‘ä¸è¿è¡Œ

### ç¼–è¯‘å‘½ä»¤

```bash
# åŸºæœ¬ç¼–è¯‘
nvcc -rdc=true -I${NVSHMEM_HOME}/include \
     -L${NVSHMEM_HOME}/lib -lnvshmem -lcuda \
     -o myapp myapp.cu

# ä½¿ç”¨ MPI å¯åŠ¨ï¼ˆæ¨èï¼‰
nvcc -rdc=true -I${NVSHMEM_HOME}/include \
     -I${MPI_HOME}/include \
     -L${NVSHMEM_HOME}/lib -L${MPI_HOME}/lib \
     -lnvshmem -lmpi -lcuda \
     -o myapp myapp.cu
```

### è¿è¡Œæ–¹å¼

#### æ–¹å¼ 1ï¼šä½¿ç”¨ mpirunï¼ˆæ¨èï¼‰

```bash
mpirun -np 4 ./myapp
```

#### æ–¹å¼ 2ï¼šä½¿ç”¨ NVSHMEM å¯åŠ¨å™¨

```bash
nvshmrun -np 4 ./myapp
```

---

## æœ€ä½³å®è·µ

### 1. å†…å­˜åˆ†é…

- âœ… ä½¿ç”¨ `nvshmem_malloc` åˆ†é…å¯¹ç§°å†…å­˜
- âœ… ç¡®ä¿æ‰€æœ‰ PE ä¼ é€’ç›¸åŒçš„ `size`
- âŒ ä¸è¦æ··ç”¨ `cudaMalloc` å’Œ `nvshmem_malloc` ç”¨äº RMA

### 2. åŒæ­¥

- âœ… ä½¿ç”¨ `nvshmem_quiet()` ç­‰å¾…è‡ªå·±çš„ Put/Get å®Œæˆ
- âœ… ä½¿ç”¨ `nvshmem_barrier_all()` å…¨å±€åŒæ­¥
- âŒ ä¸è¦å‡è®¾ Put/Get ç«‹å³å®Œæˆ

### 3. æ€§èƒ½ä¼˜åŒ–

- âœ… ä½¿ç”¨ `nvshmemx_*_block` API åˆ©ç”¨æ•´ä¸ª block
- âœ… ä½¿ç”¨éé˜»å¡æ“ä½œ (`*_nbi`) éšè—å»¶è¿Ÿ
- âœ… æ‰¹é‡ä¼ è¾“å¤§æ•°æ®å—ï¼Œå‡å°‘è°ƒç”¨æ¬¡æ•°
- âŒ é¿å…é¢‘ç¹çš„å°æ¶ˆæ¯ä¼ è¾“

### 4. è°ƒè¯•

```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
export NVSHMEM_DEBUG=TRACE

# æ£€æŸ¥ NVSHMEM é…ç½®
export NVSHMEM_INFO=1
```

---

## å‚è€ƒèµ„æº

### å®˜æ–¹æ–‡æ¡£
- [NVSHMEM Developer Page](https://developer.nvidia.com/nvshmem)
- [NVSHMEM 3.4.5 Documentation](https://docs.nvidia.com/nvshmem/api/index.html)
- [NVSHMEM API Reference](https://docs.nvidia.com/nvshmem/api/api.html)
- [NVSHMEM Examples](https://docs.nvidia.com/nvshmem/api/examples.html)

### å­¦æœ¯è®ºæ–‡
- [Redesigning GROMACS Halo Exchange (2025)](https://arxiv.org/html/2509.21527v1)
- [Dynamic Symmetric Heap Allocation in NVSHMEM](https://link.springer.com/chapter/10.1007/978-3-031-04888-3_12)
- [Evaluating One-sided Communication on CPUs and GPUs](https://dl.acm.org/doi/fullHtml/10.1145/3624062.3624182)

### GitHub èµ„æº
- [NVIDIA/nvshmem](https://github.com/NVIDIA/NVSHMEM)
- [NVSHMEM Releases](https://github.com/NVIDIA/NVSHMEM/releases)

### ç›¸å…³æŠ€æœ¯
- [PyTorch Symmetric Memory](https://docs.pytorch.org/docs/stable/symmetric_memory.html)
- [CUDA Virtual Memory Management (VMM)](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#virtual-memory-management)

---

## æ€»ç»“

NVSHMEM æä¾›äº†å¼ºå¤§çš„ GPU é—´é€šä¿¡èƒ½åŠ›ï¼š

âœ… **æ ¸å¿ƒä¼˜åŠ¿**ï¼š
- GPU ç›´æ¥å‘èµ·é€šä¿¡ï¼Œæ—  CPU å‚ä¸
- å¯¹ç§°å†…å­˜ç®€åŒ–åˆ†å¸ƒå¼ç¼–ç¨‹
- ç»†ç²’åº¦ã€é«˜å¹¶å‘é€šä¿¡
- æ˜¾è‘—ä¼˜äº MPIï¼ˆé€šä¿¡å¯†é›†å‹åœºæ™¯ï¼‰

âœ… **é€‚ç”¨åœºæ™¯**ï¼š
- MoE æ¨¡å‹é€šä¿¡ï¼ˆå¦‚ DeepEPï¼‰
- åˆ†å¸ƒå¼æ·±åº¦å­¦ä¹ 
- ç§‘å­¦è®¡ç®—ï¼ˆåˆ†å­åŠ¨åŠ›å­¦ã€CFDï¼‰
- é«˜æ€§èƒ½å›¾å¤„ç†

âœ… **å…³é”® API**ï¼š
- `nvshmem_malloc` - å¯¹ç§°å†…å­˜åˆ†é…
- `nvshmem_put/get` - è¿œç¨‹å†…å­˜è®¿é—®
- `nvshmem_barrier_all` - å…¨å±€åŒæ­¥
- `nvshmem_*_reduce` - é›†ä½“å½’çº¦

æŒæ¡ NVSHMEMï¼Œå¯ä»¥å……åˆ†å‘æŒ¥å¤š GPU é›†ç¾¤çš„é€šä¿¡æ€§èƒ½ï¼ğŸš€

---

**Sources:**
- [NVSHMEM Developer Page](https://developer.nvidia.com/nvshmem)
- [NVSHMEM 3.4.5 Documentation](https://docs.nvidia.com/nvshmem/api/introduction.html)
- [NVSHMEM Memory Management](https://docs.nvidia.com/nvshmem/api/gen/api/memory.html)
- [NVSHMEM Collective Operations](https://docs.nvidia.com/nvshmem/api/gen/api/collectives.html)
- [Redesigning GROMACS with NVSHMEM](https://arxiv.org/html/2509.21527v1)
- [Dynamic Symmetric Heap Allocation](https://link.springer.com/chapter/10.1007/978-3-031-04888-3_12)
- [Evaluating One-sided Communication Performance](https://dl.acm.org/doi/fullHtml/10.1145/3624062.3624182)
- [PyTorch Symmetric Memory](https://docs.pytorch.org/docs/stable/symmetric_memory.html)
